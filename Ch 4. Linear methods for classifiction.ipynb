{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Linear methods for classifiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Linear regression of an Indicator Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependent packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import scale\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "plt.style.use('seaborn')\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function takes two feature transform methods: normalization and MiniMaxScale\n",
    "def feature_prerpocess(data, form = 'normal'):\n",
    "    if form == \"normal\":\n",
    "        new_data = scale(data)\n",
    "    elif form == \"MinMax\":\n",
    "        new_data = minmax_scale(data)\n",
    "    return pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As name suggests we construct a response indaicator matrix instead of a vector in a linear regression model. Each of the response categories are coded via an indicator variable. Our model is of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "Y = XB\n",
    "\\end{equation}\n",
    " where $Y$ is $n \\times k$ response matrix, $X$ is $n \\times (k+1)$ matrix of observed data, $B$ is $(k+1) \\times k$ matrix of coefficients and $k$ is a number of classes/categories.<br>\n",
    "As in case of OLS,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{B} =(\\mathrm{X^TX})^{-1}X^Ty \n",
    "\\end{equation}\n",
    "\n",
    "A new observation is classified as follows:<br>\n",
    "1. estimate the fitted output $\\hat{y}=(1,x_T)\\hat{B}$, a $k$ vector\n",
    "2. classify: \n",
    "\\begin{equation}\n",
    "\\hat{G(x)} =argmax_{k \\subseteq G}\\hat{y_k}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use \"Audit\" dataset, it can be downloaded:\n",
    "* <a href=\"https://archive.ics.uci.edu/ml/datasets/Audit+Data#\">Audit Dataset\n",
    "    \n",
    "The dataset contains 777 different firms that are collected from six distinct sectors. The information about the sectors and the counts of firms are listed respectively as Irrigation (114), Public Health (77), Buildings and Roads (82), Forest (70), Corporate (47), Animal Husbandry (95), Communication (1), Electrical (4), Land (5), Science and Technology (3), Tourism (1), Fisheries (41), Industries (37), Agriculture (200). Many risk factors are examined from various areas like past records of audit office, audit-paras, environmental conditions reports, firm reputation summary, on-going issues report, profit-value records, loss-value records, follow-up reports etc. Thus we buikd a classification model that can predict the fraudulent firm on the basis the present and historical risk factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_score</th>\n",
       "      <th>PARA_A</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Risk_A</th>\n",
       "      <th>PARA_B</th>\n",
       "      <th>Score_B</th>\n",
       "      <th>Risk_B</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>numbers</th>\n",
       "      <th>Score_B.1</th>\n",
       "      <th>...</th>\n",
       "      <th>RiSk_E</th>\n",
       "      <th>History</th>\n",
       "      <th>Prob</th>\n",
       "      <th>Risk_F</th>\n",
       "      <th>Score</th>\n",
       "      <th>Inherent_Risk</th>\n",
       "      <th>CONTROL_RISK</th>\n",
       "      <th>Detection_Risk</th>\n",
       "      <th>Audit_Risk</th>\n",
       "      <th>Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.0</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.184536</td>\n",
       "      <td>2.450194</td>\n",
       "      <td>0.351289</td>\n",
       "      <td>1.351029</td>\n",
       "      <td>10.799988</td>\n",
       "      <td>0.313144</td>\n",
       "      <td>6.334008</td>\n",
       "      <td>13.218481</td>\n",
       "      <td>5.067655</td>\n",
       "      <td>0.223711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519072</td>\n",
       "      <td>0.104381</td>\n",
       "      <td>0.216753</td>\n",
       "      <td>0.053608</td>\n",
       "      <td>2.702577</td>\n",
       "      <td>17.680612</td>\n",
       "      <td>0.572680</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.168158</td>\n",
       "      <td>0.393041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.319017</td>\n",
       "      <td>5.678870</td>\n",
       "      <td>0.174055</td>\n",
       "      <td>3.440447</td>\n",
       "      <td>50.083624</td>\n",
       "      <td>0.169804</td>\n",
       "      <td>30.072845</td>\n",
       "      <td>51.312829</td>\n",
       "      <td>0.264449</td>\n",
       "      <td>0.080352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290312</td>\n",
       "      <td>0.531031</td>\n",
       "      <td>0.067987</td>\n",
       "      <td>0.305835</td>\n",
       "      <td>0.858923</td>\n",
       "      <td>54.740244</td>\n",
       "      <td>0.444581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.667494</td>\n",
       "      <td>0.488741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.370000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.583500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.890000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.214000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.570000</td>\n",
       "      <td>2.480000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.840500</td>\n",
       "      <td>7.707500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>10.663500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.249900</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.850000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1264.630000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>758.778000</td>\n",
       "      <td>1268.910000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>801.262000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>961.514400</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sector_score      PARA_A     Score_A      Risk_A       PARA_B  \\\n",
       "count    776.000000  776.000000  776.000000  776.000000   776.000000   \n",
       "mean      20.184536    2.450194    0.351289    1.351029    10.799988   \n",
       "std       24.319017    5.678870    0.174055    3.440447    50.083624   \n",
       "min        1.850000    0.000000    0.200000    0.000000     0.000000   \n",
       "25%        2.370000    0.210000    0.200000    0.042000     0.000000   \n",
       "50%        3.890000    0.875000    0.200000    0.175000     0.405000   \n",
       "75%       55.570000    2.480000    0.600000    1.488000     4.160000   \n",
       "max       59.850000   85.000000    0.600000   51.000000  1264.630000   \n",
       "\n",
       "          Score_B      Risk_B        TOTAL     numbers   Score_B.1  ...  \\\n",
       "count  776.000000  776.000000   776.000000  776.000000  776.000000  ...   \n",
       "mean     0.313144    6.334008    13.218481    5.067655    0.223711  ...   \n",
       "std      0.169804   30.072845    51.312829    0.264449    0.080352  ...   \n",
       "min      0.200000    0.000000     0.000000    5.000000    0.200000  ...   \n",
       "25%      0.200000    0.000000     0.537500    5.000000    0.200000  ...   \n",
       "50%      0.200000    0.081000     1.370000    5.000000    0.200000  ...   \n",
       "75%      0.400000    1.840500     7.707500    5.000000    0.200000  ...   \n",
       "max      0.600000  758.778000  1268.910000    9.000000    0.600000  ...   \n",
       "\n",
       "           RiSk_E     History        Prob      Risk_F       Score  \\\n",
       "count  776.000000  776.000000  776.000000  776.000000  776.000000   \n",
       "mean     0.519072    0.104381    0.216753    0.053608    2.702577   \n",
       "std      0.290312    0.531031    0.067987    0.305835    0.858923   \n",
       "min      0.400000    0.000000    0.200000    0.000000    2.000000   \n",
       "25%      0.400000    0.000000    0.200000    0.000000    2.000000   \n",
       "50%      0.400000    0.000000    0.200000    0.000000    2.400000   \n",
       "75%      0.400000    0.000000    0.200000    0.000000    3.250000   \n",
       "max      2.400000    9.000000    0.600000    5.400000    5.200000   \n",
       "\n",
       "       Inherent_Risk  CONTROL_RISK  Detection_Risk  Audit_Risk        Risk  \n",
       "count     776.000000    776.000000           776.0  776.000000  776.000000  \n",
       "mean       17.680612      0.572680             0.5    7.168158    0.393041  \n",
       "std        54.740244      0.444581             0.0   38.667494    0.488741  \n",
       "min         1.400000      0.400000             0.5    0.280000    0.000000  \n",
       "25%         1.583500      0.400000             0.5    0.316700    0.000000  \n",
       "50%         2.214000      0.400000             0.5    0.555600    0.000000  \n",
       "75%        10.663500      0.400000             0.5    3.249900    1.000000  \n",
       "max       801.262000      5.800000             0.5  961.514400    1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download data \n",
    "Audit = pd.read_csv('audit_risk.csv', sep=',', encoding= 'unicode_escape')\n",
    "Audit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 776 entries, 0 to 775\n",
      "Data columns (total 27 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Sector_score    776 non-null    float64\n",
      " 1   LOCATION_ID     776 non-null    object \n",
      " 2   PARA_A          776 non-null    float64\n",
      " 3   Score_A         776 non-null    float64\n",
      " 4   Risk_A          776 non-null    float64\n",
      " 5   PARA_B          776 non-null    float64\n",
      " 6   Score_B         776 non-null    float64\n",
      " 7   Risk_B          776 non-null    float64\n",
      " 8   TOTAL           776 non-null    float64\n",
      " 9   numbers         776 non-null    float64\n",
      " 10  Score_B.1       776 non-null    float64\n",
      " 11  Risk_C          776 non-null    float64\n",
      " 12  Money_Value     775 non-null    float64\n",
      " 13  Score_MV        776 non-null    float64\n",
      " 14  Risk_D          776 non-null    float64\n",
      " 15  District_Loss   776 non-null    int64  \n",
      " 16  PROB            776 non-null    float64\n",
      " 17  RiSk_E          776 non-null    float64\n",
      " 18  History         776 non-null    int64  \n",
      " 19  Prob            776 non-null    float64\n",
      " 20  Risk_F          776 non-null    float64\n",
      " 21  Score           776 non-null    float64\n",
      " 22  Inherent_Risk   776 non-null    float64\n",
      " 23  CONTROL_RISK    776 non-null    float64\n",
      " 24  Detection_Risk  776 non-null    float64\n",
      " 25  Audit_Risk      776 non-null    float64\n",
      " 26  Risk            776 non-null    int64  \n",
      "dtypes: float64(23), int64(3), object(1)\n",
      "memory usage: 163.8+ KB\n"
     ]
    }
   ],
   "source": [
    "Audit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is no missing values in the data. The dataset have 26 feature variables and 1 cateogrical response variable. We drop \"LOCATION_ID\" feature as it has no a predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature matrix\n",
    "X_matrix = Audit[Audit.columns[~Audit.columns.isin([\"LOCATION_ID\", 'Risk'])]]\n",
    "X_matrix = X_matrix.fillna(method ='pad') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of 1's in the design matrix allows estimation of the y-intercept, so we add a new column to our data matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix.insert(0,'intercept',np.ones((len(X_matrix),)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response indicator matrix\n",
    "Y = Audit[[\"Risk\"]]\n",
    "Y_var = Audit[[\"Risk\"]]\n",
    "Y = Y.astype('str') \n",
    "Y_matrix = pd.get_dummies(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute OLS estimator for $B$ matrix of paramaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Not fraudulent firm</th>\n",
       "      <th>fraudulent firm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.040734</td>\n",
       "      <td>-0.714099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061286</td>\n",
       "      <td>-0.061286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.933510</td>\n",
       "      <td>-1.036901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020361</td>\n",
       "      <td>-0.020361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.084466</td>\n",
       "      <td>0.084466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.622363</td>\n",
       "      <td>-4.185401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.261808</td>\n",
       "      <td>-0.261808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.012569</td>\n",
       "      <td>-0.012569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.027185</td>\n",
       "      <td>-0.027185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.846742</td>\n",
       "      <td>-3.285713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.138497</td>\n",
       "      <td>-0.138497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.062169</td>\n",
       "      <td>0.062169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.505094</td>\n",
       "      <td>-0.392666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.248011</td>\n",
       "      <td>-0.248011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006704</td>\n",
       "      <td>0.004626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.731748</td>\n",
       "      <td>-0.700383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.635688</td>\n",
       "      <td>-20.783001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.018268</td>\n",
       "      <td>-0.018268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.922760</td>\n",
       "      <td>-1.776782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.569722</td>\n",
       "      <td>-20.391043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.455342</td>\n",
       "      <td>1.629858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.145210</td>\n",
       "      <td>0.145210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-19.817356</td>\n",
       "      <td>20.771460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.198972</td>\n",
       "      <td>1.310060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003508</td>\n",
       "      <td>-0.003508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Not fraudulent firm  fraudulent firm\n",
       "0              1.040734        -0.714099\n",
       "1              0.001004        -0.001004\n",
       "2              0.061286        -0.061286\n",
       "3              0.933510        -1.036901\n",
       "4              0.020361        -0.020361\n",
       "5             -0.084466         0.084466\n",
       "6              4.622363        -4.185401\n",
       "7              0.261808        -0.261808\n",
       "8              0.012569        -0.012569\n",
       "9              0.027185        -0.027185\n",
       "10             3.846742        -3.285713\n",
       "11             0.138497        -0.138497\n",
       "12            -0.062169         0.062169\n",
       "13             0.505094        -0.392666\n",
       "14             0.248011        -0.248011\n",
       "15             0.006704         0.004626\n",
       "16             0.731748        -0.700383\n",
       "17            20.635688       -20.783001\n",
       "18             0.018268        -0.018268\n",
       "19             1.922760        -1.776782\n",
       "20            19.569722       -20.391043\n",
       "21            -1.455342         1.629858\n",
       "22            -0.145210         0.145210\n",
       "23           -19.817356        20.771460\n",
       "24            -0.198972         1.310060\n",
       "25             0.003508        -0.003508"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paramater estimation:\n",
    "beta = (linalg.inv(X_matrix.T@X_matrix))@X_matrix.T@Y_matrix\n",
    "beta.columns  = ['Not fraudulent firm', 'fraudulent firm']\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of fitted values\n",
    "D = X_matrix@beta.to_numpy()\n",
    "Y_fitted = pd.DataFrame(np.where(D.iloc[:,0]<D.iloc[:,1], 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rate (ERR) is calculated as the number of all incorrect predictions divided by the total number of the dataset:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{FP+FN}{P+N}\n",
    "\\end{equation}\n",
    "\n",
    "Error rate is calculated as the total number of two incorrect predictions (FN + FP) divided by the total number of a dataset (P + N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20231958762886598"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate error rate\n",
    "Error_rate = np.sum(np.abs(Y_fitted.iloc[:,0]-Y_var.iloc[:,0]))/len(Y_fitted)\n",
    "Error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior probability of $P(G|X)$ is given by Bayes theorem:\n",
    "\n",
    "\\begin{equation}\n",
    "P(G=k|X=x) = \\frac{f_k(x)\\pi_k}{\\sum_{l=1}^{K} f_l(x)\\pi_l}\n",
    "\\end{equation}\n",
    "\n",
    "where, $f_k(x)$ is the conditional density of $X$ in class $G=k$, and $\\pi_k$ is the prior probability  of class $k$, with $\\sum_{l=1}^{K}\\pi_l=1$ <br>\n",
    "In LDA we assume:\n",
    "1. each class density is Normally distributed,\n",
    "2. the classes have a common variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have p variables then each class has a multivariate Normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "f_k(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)}\n",
    "\\end{equation}\n",
    "\n",
    "As a decision boundary we use log-ratio, which is linear in $x$ (that's why it is called Linear Discriminant Analysis):\n",
    "\n",
    "\\begin{equation}\n",
    "log\\frac{P(G=k|X=x)}{P(G=l|X=x)} = log\\frac{f_k(x)}{f_l(x)}+log\\frac{\\pi_k}{\\pi_l}=log\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(\\mu_k-\\mu_l)^T\\Sigma^{-1}(\\mu_k-\\mu_l)+x^T\\Sigma^{-1}(\\mu_k-\\mu_l)\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_k(x)=x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}(\\mu_k)^T\\Sigma^{-1}(\\mu_k)+log\\pi_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not know the parameters of the Normal distributions, and we will estimate them using our training data:\n",
    "- $\\hat{\\pi_k}=\\frac{N_k}{N}$, where $N_k$ is the number of <em>class-k</em> observations;\n",
    "- $\\hat{\\mu_k}=\\sum_{g_i=k}^{}/N_k$;\n",
    "- $\\hat{\\Sigma}=\\frac{\\sum_{k=k}^{K}\\sum_{g_i=k}^{}(x_i-\\hat{\\mu_k})^T}{(N-K)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop intercept from the data matrix\n",
    "X_matrix = X_matrix.drop(['intercept'], axis=1)\n",
    "X_matrix = feature_prerpocess(X_matrix, form = 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimation of the paramaters of Normal distributions\n",
    "# 1) Prior probabilities\n",
    "prior_prob = pd.DataFrame({\"fraudulent firm\": np.sum(Y_var)/len(Y_var), \"Not fraudulent firm\":(len(Y_var)-np.sum(Y_var))/len(Y_var)})\n",
    "\n",
    "#2) means\n",
    "fraudulent_firm = pd.concat([X_matrix,Y_var],axis=1)[pd.concat([X_matrix,Y_var],axis=1)[\"Risk\"]==1]\n",
    "Not_fraudulent_firm = pd.concat([X_matrix,Y_var],axis=1)[pd.concat([X_matrix,Y_var],axis=1)[\"Risk\"]==0]\n",
    "means_class = pd.DataFrame({\"fraudulent firm\": np.mean(fraudulent_firm.iloc[:,:-1]), \"Not fraudulent firm\":np.mean(Not_fraudulent_firm.iloc[:,:-1])})\n",
    "\n",
    "#3) Covariance matrix\n",
    "covariance_fraudulent_firm = np.subtract(fraudulent_firm.iloc[:,:-1], means_class[['fraudulent firm']].T).T@np.subtract(fraudulent_firm.iloc[:,:-1], means_class[['fraudulent firm']].T)\n",
    "covariance_not_fraudulent_firm = np.subtract(Not_fraudulent_firm.iloc[:,:-1], means_class[['Not fraudulent firm']].T).T@np.subtract(Not_fraudulent_firm.iloc[:,:-1], means_class[['Not fraudulent firm']].T)\n",
    "Covarianc_matrix = (covariance_fraudulent_firm+covariance_not_fraudulent_firm)/(len(X_matrix)-X_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear discriminant function\n",
    "Discriminant_score = X_matrix@linalg.pinv(Covarianc_matrix)@means_class.to_numpy()-np.diag((1/2)*means_class.to_numpy().T@means_class)+np.log(prior_prob).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "Y_fitted = pd.DataFrame(np.where(Discriminant_score.iloc[:,0]>Discriminant_score.iloc[:,1], 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04896907216494845"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate error rate\n",
    "Error_rate = np.sum(np.abs(Y_fitted.iloc[:,0]-Y_var.iloc[:,0]))/len(Y_fitted)\n",
    "Error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike LDA, QDA assumes that each class has it's own covariance matrix. That is, it assumes that an observation from the $k^{th}$ class is of the form $X \\sim N(\\mu_k, \\Sigma_k)$. Under this assumtion our discriminant function is of the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_k(x)=-\\frac{1}{2}x^T\\Sigma_k^{-1}x+x^T\\Sigma_k^{-1}\\mu_k-\\frac{1}{2}(\\mu_k)^T\\Sigma_k^{-1}(\\mu_k) -\\frac{1}{2}log|\\Sigma_k| +log\\pi_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimation of the paramaters of Normal distributions\n",
    "means_class_fraudulent_firm = np.mean(fraudulent_firm.iloc[:,:-1])\n",
    "means_class_Not_fraudulent_firm = np.mean(Not_fraudulent_firm.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Covarianc_matrix = [covariance_fraudulent_firm, covariance_not_fraudulent_firm]\n",
    "means_class = [means_class_fraudulent_firm,means_class_Not_fraudulent_firm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear discriminant function for two classes\n",
    "\n",
    "discriminant_score_QDA = [0]*2\n",
    "for k in range(2):\n",
    "    squared_term = (1/2)*X_matrix@linalg.pinv(Covarianc_matrix[k])@X_matrix.T\n",
    "    linear_term = X_matrix@linalg.pinv(Covarianc_matrix[k])@means_class[k].to_numpy()\n",
    "    third_term = (1/2)*means_class[k]@linalg.pinv(Covarianc_matrix[k])@means_class[k].to_numpy()\n",
    "    last_terms = -(1/2)*linalg.det(Covarianc_matrix[k])+prior_prob.iloc[:,k]\n",
    "    discriminant_score_QDA[k] = np.diag(-squared_term+linear_term-third_term+last_terms[0])\n",
    "\n",
    "discriminant_score_QDA= pd.DataFrame(discriminant_score_QDA).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "Y_fitted_QDA = pd.DataFrame(np.where(discriminant_score_QDA.iloc[:,0]>discriminant_score_QDA.iloc[:,1], 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045103092783505154"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate error rate\n",
    "Error_rate_QDA = np.sum(np.abs(Y_fitted_QDA.iloc[:,0]-Y_var.iloc[:,0]))/len(Y_fitted_QDA)\n",
    "Error_rate_QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1:  Regularized Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized covariance matrices have the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\Sigma_k}(\\alpha) = \\alpha\\hat{\\Sigma_k} + (1-\\alpha)\\hat{\\Sigma}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{\\Sigma}$ is the pooled covariance matrix used in LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9394105894105894"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regularized Discriminant Analysis\n",
    "Reg_LDA = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "# model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "Reg_LDA_scores = cross_val_score(Reg_LDA, X_matrix, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "Reg_LDA_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4:  Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1:  Simple Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model has the form:\n",
    "\\begin{equation}\n",
    "log(\\frac{Pr(G=K-1|X=x)}{Pr(G=K|X=x)}) = \\beta_{(K-1)0}+\\beta_{K-1}^Tx\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "Log_reg = LogisticRegression(penalty='none', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897047397047399"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "Log_reg_score = cross_val_score(Log_reg, X_matrix, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "Log_reg_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2:  Regularized Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L_1z$ penalty can be used for variable selection and shrinkage with any linear regression model including logistic regression:\n",
    "\n",
    "\\begin{equation}\n",
    "max_{\\beta_0, \\beta}=\\{ \\sum_{i=1}^{N} [y_i(\\beta_0+\\beta^Tx_i)-log(1+e^{\\beta_0+\\beta^Tx_i})]-\\lambda \\sum_{j=1}^{p} |\\beta_j| \\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-1 Logistic regression\n",
    "Log_reg_l1 = LogisticRegression(penalty='l2', solver = \"sag\", random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746198246198249"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "Log_reg_l1_score = cross_val_score(Log_reg_l1, X_matrix, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "Log_reg_l1_score.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
